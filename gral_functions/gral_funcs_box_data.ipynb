{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPpQaW9nyAlb0SfdaNI9Rv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LwJ8Xurb9vOT"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle as pkl\n","from sklearn.model_selection import cross_validate, cross_val_predict, learning_curve\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n","\n","\n","def read_gsheet_shared(sheet_id, sheet_name):\n","  '''\n","  Function to read gsheet as a dataframe.\n","  Parameters:\n","    sheet_id (string): Example \"2y1kdVoBif48Kq8s7aUeBEQ4Omzvo3y1aqi495LOZVme\"\n","    sheet_name (string): Name of sheet, example \"abril\"\n","  Returns:\n","    A dataframe of the csv in the gsheet.\n","  '''\n","  id = sheet_id\n","  name = sheet_name\n","  gsheet_url = 'https://docs.google.com/spreadsheets/d/{}/gviz/tq?tqx=out:csv&sheet={}'.format(id, name)\n","  df = pd.read_csv(gsheet_url)\n","  return df\n","\n","\n","def cross_val(model, X, y, cv=5, figsize=(5,3)):\n","  '''\n","  Function to cross val ml models and check results.\n","  Parameters:\n","    :model: ml model.\n","    :X: predictor data.\n","    :y: data to predict.\n","    :cv: cross val folders, default=5.\n","    :figsize: Size of the plots, default=(5,3).\n","  Returns:\n","    :print: métricas de resultados.\n","    :plot: Confusion matrix, roc curve, precision recall curve, learning curve.\n","  '''\n","  cv_results = cross_validate(model, X, y, cv=cv)\n","\n","  print(f'Fit time mean: {cv_results[\"fit_time\"].mean()}')\n","  print(f'Score time mean: {cv_results[\"score_time\"].mean()}')\n","  print(f'Test score: {cv_results[\"test_score\"]}')\n","  print(f'Test mean score: {cv_results[\"test_score\"].mean()}')\n","\n","  y_pred = cross_val_predict(model, X, y, cv=cv)\n","  conf_matrix = confusion_matrix(y, y_pred)\n","  plt.figure(figsize=figsize)\n","  sns.heatmap(conf_matrix, annot=True, fmt='d')\n","  plt.title('Confusion Matrix')\n","  plt.ylabel('True')\n","  plt.xlabel('Predicted')\n","  plt.show()\n","\n","  fpr, tpr, _ = roc_curve(y, y_pred)\n","  roc_auc = auc(fpr, tpr)\n","  plt.figure(figsize=figsize)\n","  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver Operating Characteristic')\n","  plt.legend(loc='lower right')\n","  plt.show()\n","\n","  precision, recall, _ = precision_recall_curve(y, y_pred)\n","  plt.figure(figsize=figsize)\n","  plt.step(recall, precision, color='b', alpha=0.2, where='post')\n","  plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n","  plt.xlabel('Recall')\n","  plt.ylabel('Precision')\n","  plt.title('Precision-Recall curve')\n","  plt.show()\n","\n","  train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv)\n","  train_scores_mean = np.mean(train_scores, axis=1)\n","  train_scores_std = np.std(train_scores, axis=1)\n","  test_scores_mean = np.mean(test_scores, axis=1)\n","  test_scores_std = np.std(test_scores, axis=1)\n","\n","  plt.figure(figsize=figsize)\n","  plt.title(\"Learning Curve\")\n","  plt.xlabel(\"Training examples\")\n","  plt.ylabel(\"Score\")\n","  plt.grid()\n","  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                   train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n","  plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                   test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n","  plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n","           label=\"Training score\")\n","  plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n","           label=\"Cross-validation score\")\n","  plt.legend(loc=\"best\")\n","  plt.show()\n","\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class Data_clusterer(BaseEstimator, TransformerMixin):\n","    def __init__(self, model):\n","        '''\n","        Constructor method.\n","        Parameters:\n","          :model: Unfitted clustering model.\n","        Returns:\n","          None\n","        '''\n","        self.model = model\n","\n","    def fit(self, X, y=None):\n","        '''\n","        Fit the clustering model to the provided data.\n","        Parameters:\n","          :X: Data used to fit the clustering model.\n","        Returns:\n","          None\n","        '''\n","        self.model.fit(X)\n","        return self\n","\n","    def transform(self, X):\n","        '''\n","        Use the fitted clustering model to predict.\n","        Parameters:\n","          :X: Dataframe to predict clusters for.\n","        Returns:\n","          Dataframe with a 'cluster' column attached.\n","        '''\n","        clusters = self.model.predict(X)\n","        clusters = pd.Series(clusters, index=X.index, name='cluster')\n","        temp = X.merge(clusters, how='outer', left_index=True, right_index=True)\n","        return temp\n","\n","\n","class Model_applied(BaseEstimator, TransformerMixin):\n","    def __init__(self, model):\n","        '''\n","        Constructor method.\n","        Parameters:\n","          :model: Unfitted classification model.\n","        Returns:\n","          None\n","        '''\n","        self.model = model\n","\n","    def fit(self, X, y):\n","        '''\n","        Fit the classification model to the provided data.\n","        Parameters:\n","          :X: Data used to fit the classification model.\n","          :y: Target labels.\n","        Returns:\n","          None\n","        '''\n","        self.model.fit(X, y)\n","        return self\n","\n","    def transform(self, X):\n","        '''\n","        Use the fitted classification model to predict and return probabilities.\n","        Parameters:\n","          :X: DataFrame to make predictions on.\n","        Returns:\n","          DataFrame with prediction and, if available, probability columns.\n","        '''\n","        df_pred = pd.DataFrame()\n","        pred_new = self.model.predict(X)\n","        df_pred['boxer1_pred'] = pred_new\n","\n","        try:\n","            probabilities = self.model.predict_proba(X)\n","            df_pred['prob_win'] = probabilities[:, 1]  # Assuming class 1 represents \"win.\"\n","            df_pred['prob_loss'] = probabilities[:, 0]  # Assuming class 0 represents \"loss.\"\n","        except AttributeError as e:\n","            # Handle the case where the model does not support probability estimation.\n","            print(f\"Warning: {type(self.model).__name__} does not support probability estimation. \"\n","                  \"Probability columns will not be available.\")\n","        return df_pred\n","\n","\n","def check_fails_and_probas(df_cluster, y_true, y_pred, prob_loss, prob_win, figsize=(5,3)):\n","  '''\n","  Función para revisar errores, aciertos y probabilidades de modelo, según sus respectivos cluster.\n","  Parameters:\n","    :df_cluster: dataframe con columna de clusters.\n","    :y_true: y verdadero.\n","    :y_pred: y predicción.\n","    :prob_loss: probabilidad boxer 1 pierde.\n","    :prob_win: probabilidad boxer 1 gana.\n","    :figsize=(5,3): figsize de las gráficas.\n","  Returns:\n","    :return: df cluster con columnas true res, pred res, goodpred, prob loss, prob win.\n","    :plot: conteo de falsos por verdaderos según clusters, Perc false / total by Prob win, Perc. false / total for the clusters.\n","    :print: porcentaje de falsos por verdaderos según cada cluster.\n","  '''\n","  dfx = df_cluster.copy()\n","  dfx['true_res'] = y_true.values\n","  dfx['pred_res'] = y_pred.values\n","  dfx['goodpred'] = (dfx.true_res == dfx.pred_res).values\n","  dfx['prob_loss'] = prob_loss.values\n","  dfx['prob_win'] = prob_win.values\n","\n","  # Count the occurrences of each cluster and goodpred combination\n","  counts = dfx.groupby(['cluster', 'goodpred']).size().reset_index(name='count')\n","\n","  plt.figure(figsize=figsize)\n","  plt.bar(counts.index, counts['count'], color='white', edgecolor='blue', linewidth=2.5)\n","  labels = [f'Cluster {c}, GoodPred {g}' for c, g in zip(counts['cluster'], counts['goodpred'])]\n","  plt.xticks(counts.index, labels, rotation=90)\n","  plt.xlabel('Cluster and GoodPred')\n","  plt.ylabel('Count')\n","  plt.title('Data Counts by Cluster and GoodPred')\n","  plt.show()\n","\n","  # perc false per true by clusters\n","  titulo = 'Porcentaje de False per Trues by clusters'\n","  print(titulo+'\\n'+(len(titulo)*'='))\n","  perc_false_per_true_by_cluster = counts.groupby('cluster').apply(lambda x: x[x['goodpred'] == False]['count'].sum() / x[x['goodpred'] == True]['count'].sum())\n","  print(perc_false_per_true_by_cluster)\n","\n","  a = round(dfx.prob_win,1)\n","  b = dfx[['goodpred','prob_win']]\n","  b['prob_win'] = a\n","  b = pd.get_dummies(b, columns=['goodpred'])\n","  b = b.groupby('prob_win').sum()\n","  b[['goodpred_False','goodpred_True']] = b[['goodpred_False','goodpred_True']].astype(float)\n","  b['false_over_total'] = round(b.goodpred_False / (b.goodpred_False + b.goodpred_True), 2)\n","  b['true_over_total'] = round(b.goodpred_True / (b.goodpred_False + b.goodpred_True), 2)\n","\n","  fig, ax = plt.subplots(figsize=figsize)\n","  sns.barplot(x=b.index, y=b.false_over_total, color='white', edgecolor='red', linewidth=1.5, label='false')\n","  sns.pointplot(x=b.index, y=b.true_over_total, color='#6B7FFF', label='true')\n","  ax.set_ylim(0,1)\n","  plt.legend()\n","  plt.ylabel('Percentage')\n","  plt.xlabel('Prob. win ex.(0.5 = from 0.46 to 0.55)')\n","  plt.title('Perc False - true / total by Prob win')\n","  plt.show()\n","\n","  clusters = dfx.cluster.unique()\n","  for i in clusters:\n","    temp1 = dfx.copy()\n","    temp1 = temp1[['goodpred','cluster','prob_win']].query(f'cluster == {i}')\n","    temp1['prob_win'] = round(temp1.prob_win, 1)\n","    temp1 = pd.get_dummies(temp1, columns=['goodpred'])\n","    temp1 = temp1.groupby(['cluster','prob_win']).sum().reset_index()\n","    try:\n","        temp1[['goodpred_False','goodpred_True']] = temp1[['goodpred_False','goodpred_True']].astype(float)\n","        temp1['false_over_total'] = round(temp1.goodpred_False / (temp1.goodpred_True + temp1.goodpred_False), 2)\n","        temp1['true_over_total'] = round(temp1.goodpred_True / (temp1.goodpred_False + temp1.goodpred_True), 2)\n","\n","        fig, ax = plt.subplots(figsize=figsize)\n","        sns.barplot(x=temp1.prob_win, y=temp1.false_over_total, color='white', edgecolor='red', linewidth=1.5, label='false')\n","        sns.pointplot(x=temp1.prob_win, y=temp1.true_over_total, color='#6B7FFF', label='true')\n","        ax.set_ylim(0,1)\n","        plt.legend()\n","        plt.ylabel('Percentage')\n","        plt.xlabel('Prob. Win ex.(0.5 = from 0.46 to 0.55)')\n","        plt.title(f'False-true / total by probab. Cluster {i}')\n","        plt.show()\n","    except (AttributeError, KeyError):\n","        print(f'\\nError in cluster: {i}, maybe there are no fails\\n')\n","\n","  return dfx\n","\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","\n","def plot_clusters(clusters, dim_reduct_values, cmap=\"Set2\", figsize=(5, 3)):\n","    \"\"\"\n","    Plots the projection of the features colored by clusters.\n","    Parameters:\n","        clusters (numpy array): The clusters of the data.\n","        dim_reduct_values (numpy array): The dimensionality-reduced values of features.\n","        cmap (str or colormap, optional): The colormap for coloring clusters. Default is \"Set2\".\n","        figsize (tuple, optional): The size of the plot. Default is (5, 3).\n","    Returns:\n","        None (displays the plot).\n","    \"\"\"\n","    cmap = plt.get_cmap(cmap)\n","    n_clusters = np.unique(clusters).shape[0]\n","    fig, ax = plt.subplots(figsize=figsize)\n","    scatter = ax.scatter(dim_reduct_values[:, 0], dim_reduct_values[:, 1],\n","                         c=[cmap(x / n_clusters) for x in clusters], s=40, alpha=.4)\n","\n","    # Calculate and plot the cluster centers as text\n","    for cluster_id in range(n_clusters):\n","        cluster_points = dim_reduct_values[clusters == cluster_id]\n","        cluster_center = np.mean(cluster_points, axis=0)\n","        ax.text(cluster_center[0], cluster_center[1], str(cluster_id), fontsize=12,\n","                ha='center', va='center', color='black', weight='bold')\n","    plt.title('dim reduct projection of values, colored by clusters', fontsize=14)\n","    plt.show()\n","\n","\n","def find_optimal_clusters(data, scaler=StandardScaler(), max_clusters=10, clustering_model=KMeans, figsize=(5,3)):\n","    \"\"\"\n","    Function to find the optimal number of clusters using the Elbow Method.\n","    Parameters:\n","        data (numpy.ndarray or pandas.DataFrame): The dataset to be analyzed.\n","        scaler (optional): The scaler for the values. Default is StandardScaler.\n","        max_clusters (int, optional): The maximum number of clusters to consider.\n","        clustering_model (optional): The clustering model to use. Default is KMeans.\n","        figsize (tuple, optional): The size of the plot. Default is (5,3).\n","    Returns:\n","        None (plots the Elbow Method graph).\n","    \"\"\"\n","    # Standardize the data to have zero mean and unit variance\n","    standardized_data = scaler.fit_transform(data)\n","\n","    # Initialize an empty list to store the within-cluster sum of squares\n","    wcss = []\n","\n","    # Calculate WCSS for different number of clusters from 1 to max_clusters\n","    for num_clusters in range(1, max_clusters + 1):\n","        model = clustering_model(n_clusters=num_clusters, random_state=42)\n","        model.fit(standardized_data)\n","        wcss.append(model.inertia_)  # Sum of squared distances to the closest cluster center\n","\n","    # Plot the Elbow Method graph\n","    plt.figure(figsize=figsize)\n","    plt.plot(range(1, max_clusters + 1), wcss, marker='o')\n","    plt.xlabel('Number of Clusters')\n","    plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n","    plt.title('Elbow Method to Find Optimal Number of Clusters')\n","    plt.xticks(np.arange(1, max_clusters + 1))\n","    plt.grid()\n","    plt.show()\n","\n","\n","def convert_dec_val_to_probs(df, scaler):\n","  '''\n","  Function to convert decision values to class probabilities.\n","  This function takes a DataFrame with decision values and a scaler (e.g., MinMaxScaler)\n","  and converts the decision values to class probabilities.\n","  It handles both positive and negative decision values separately, scales them,\n","  and computes class probabilities for a binary classification task.\n","  The resulting DataFrame contains columns 'prob_win' and 'prob_loss' representing\n","  the probabilities of winning and losing, respectively.\n","    Parameters:\n","      :df: DataFrame containing decision values ('dec_val' column).\n","      :scaler: A scaler (min max scaler) used for scaling the decision values.\n","    Returns:\n","      DataFrame with 'prob_win' and 'prob_loss' columns representing class probabilities.\n","  '''\n","  dfneg = df[df.dec_val < 0]\n","  dfneg['abs_dec_val'] = abs(dfneg.dec_val)\n","  dfneg['neg_scal'] = scaler.fit_transform(dfneg.abs_dec_val.values.reshape(-1,1))\n","  print(f'Min val scaled (negative): {scaler.data_min_}, max val scaled (negative): {scaler.data_max_}')\n","  dfneg['neg_scal'] = -(dfneg.neg_scal)\n","  dfpos = df[df.dec_val > 0]\n","  dfpos['pos_scal'] = scaler.fit_transform(dfpos.dec_val.values.reshape(-1,1))\n","  print(f'Min val scaled (positive): {scaler.data_min_}, max val scaled (positive): {scaler.data_max_}')\n","  data = pd.concat([dfneg, dfpos], axis=1).sort_index()\n","  data.loc[data.neg_scal.isna(), 'neg_scal'] = data.loc[data.neg_scal.isna(), 'pos_scal']\n","  data['prob_win'] = scaler.fit_transform(data.neg_scal.values.reshape(-1, 1))\n","  data['prob_loss'] = 1 - data.prob_win\n","  data = data[['prob_win','prob_loss']]\n","  temp = pd.concat([df, data], axis=1)\n","  temp = temp[['boxer1_pred','prob_win','prob_loss','initial_index','cluster']]\n","  return temp"]}]}